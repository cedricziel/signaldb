# SignalDB Distribution Configuration
# Copy to signaldb.toml and adjust for your environment

[database]
# Database for internal metadata storage
dsn = "sqlite://.data/signaldb.db"
# dsn = "postgres://user:password@localhost/signaldb"

[auth]
# Multi-Tenancy Authentication Configuration
# Controls API key validation and tenant isolation
enabled = true  # Set to false to disable authentication (dev/testing only)

# Admin API key for the /api/v1/admin/* management endpoints
# Generate a strong random key for production use
# admin_api_key = "sk-admin-your-secret-key-here"

# Tenant definitions with API keys and datasets
# Each tenant is isolated with separate WAL paths and Iceberg catalogs

# Example tenant: Acme Corporation
[[auth.tenants]]
id = "acme"
slug = "acme"
name = "Acme Corporation"
default_dataset = "production"  # Used when X-Dataset-ID header is not provided

# API keys for the Acme tenant
# Each key can have a descriptive name for auditing
[[auth.tenants.api_keys]]
key = "sk-acme-prod-key-123"       # The actual API key used in Authorization: Bearer header
name = "Production Key"             # Human-readable name for this key

[[auth.tenants.api_keys]]
key = "sk-acme-staging-key-456"
name = "Staging Key"

# Datasets for the Acme tenant
# Tenants can have multiple datasets for environment isolation
[[auth.tenants.datasets]]
id = "production"
slug = "production"
is_default = true

[[auth.tenants.datasets]]
id = "staging"
slug = "staging"
is_default = false

# Example tenant: Beta Inc
[[auth.tenants]]
id = "beta"
slug = "beta"
name = "Beta Inc"
default_dataset = "default"

[[auth.tenants.api_keys]]
key = "sk-beta-key-789"
name = "Beta Production Key"

[[auth.tenants.datasets]]
id = "default"
slug = "default"
is_default = true

# Optional: Custom schema configuration per tenant
# [[auth.tenants.schema_config]]
# catalog_type = "sql"
# catalog_uri = "sqlite://.data/acme_catalog.db"

# Authentication Headers Required:
# - Authorization: Bearer <api-key>
# - X-Tenant-ID: <tenant-id>
# - X-Dataset-ID: <dataset-id> (optional, uses default_dataset if not provided)
#
# Example authenticated OTLP request:
#   curl -X POST http://localhost:4318/v1/traces \
#     -H "Authorization: Bearer sk-acme-prod-key-123" \
#     -H "X-Tenant-ID: acme" \
#     -H "X-Dataset-ID: production" \
#     -H "Content-Type: application/json" \
#     -d @traces.json

[storage]
# Object storage for data files (Parquet, etc.)
dsn = "file:///.data/storage"
# dsn = "memory://"                    # In-memory storage for testing
# dsn = "s3://my-bucket/signaldb/"     # S3 storage (future)

[schema]
# Iceberg catalog configuration for table metadata
catalog_type = "sql"                   # sql (recommended) or memory
catalog_uri = "sqlite::memory:"        # In-memory SQLite catalog
# catalog_uri = "sqlite://.data/catalog.db"  # Persistent SQLite catalog
# catalog_uri = "postgres://user:password@localhost:5432/iceberg"  # Production PostgreSQL

# Default schemas to create for new tenants
[schema.default_schemas]
traces_enabled = true
logs_enabled = true
metrics_enabled = true

# Custom schema definitions (optional)
# [schema.default_schemas.custom_schemas]
# my_custom_table = '''{"fields": [...]}'''

# Advanced Iceberg Configuration (all settings are optional and auto-optimized)
# These provide fine-grained control for specialized workloads

# [schema.optimization]
# # Batch processing optimization (automatic by default)
# max_rows_per_batch = 50000           # Maximum rows per batch before splitting
# max_memory_per_batch_mb = 128        # Maximum memory per batch in MB
# enable_auto_split = true             # Automatically split oversized batches
# target_concurrent_batches = 4        # Number of batches to process concurrently
# 
# # Catalog caching for performance (enabled by default)
# enable_catalog_caching = true        # Cache catalog connections
# catalog_cache_ttl_seconds = 300      # Cache TTL in seconds (5 minutes)
#
# # Connection pooling for production (automatic by default)
# min_connections = 2                  # Minimum pool connections
# max_connections = 10                 # Maximum pool connections
# connection_timeout_ms = 5000         # Connection timeout
# idle_timeout_seconds = 300           # Idle connection timeout
# max_lifetime_seconds = 1800          # Connection max lifetime
#
# # Retry logic for reliability (enabled by default)
# max_retry_attempts = 3               # Maximum retry attempts
# initial_retry_delay_ms = 100         # Initial retry delay
# max_retry_delay_seconds = 5          # Maximum retry delay
# retry_backoff_multiplier = 2.0       # Exponential backoff multiplier

[discovery]
# Service discovery configuration
dsn = "sqlite::memory:"               # In-memory service registry
heartbeat_interval = "30s"
poll_interval = "60s"
ttl = "300s"

[wal]
# Write-Ahead Log configuration
wal_dir = ".data/wal"
max_segment_size = 67108864           # 64MB
max_buffer_entries = 1000
flush_interval = "30s"
max_buffer_size_bytes = 134217728     # 128MB

[compactor]
# Compactor service configuration (Phase 2: Compaction + Phase 3: Retention)
enabled = false                       # Enable compactor service
tick_interval = "5m"                  # Interval between compaction planning cycles
target_file_size_mb = 128             # Target file size after compaction (MB)
file_count_threshold = 5              # Minimum files to trigger compaction
min_input_file_size_kb = 1024         # Minimum input file size (KB)
max_files_per_job = 20                # Maximum files per compaction job

# Phase 3: Retention enforcement configuration
[compactor.retention]
enabled = false                       # Enable retention enforcement (opt-in)
retention_check_interval = "1h"       # Interval between retention checks
grace_period = "1h"                   # Safety margin before dropping expired data
timezone = "UTC"                      # Timezone for retention calculations

# Default retention periods per signal type
traces = "7d"                         # Keep traces for 7 days
logs = "30d"                          # Keep logs for 30 days
metrics = "90d"                       # Keep metrics for 90 days

# Tenant-specific retention overrides
# [compactor.retention.tenant_overrides.acme]
# traces = "14d"                      # Override: Keep Acme traces for 14 days
# logs = "60d"                        # Override: Keep Acme logs for 60 days
#
# # Dataset-specific retention overrides
# [compactor.retention.tenant_overrides.acme.dataset_overrides.production]
# traces = "30d"                      # Override: Keep production traces for 30 days
# logs = "90d"                        # Override: Keep production logs for 90 days
#
# [compactor.retention.tenant_overrides.acme.dataset_overrides.staging]
# traces = "2d"                       # Override: Keep staging traces for only 2 days

[tenants]
# Multi-tenant configuration (always active)
default_tenant = "default"

# Example tenant configuration
# [tenants.tenants.tenant1]
# enabled = true
# [tenants.tenants.tenant1.schema]
# catalog_type = "sql"
# catalog_uri = "sqlite://.data/tenant1_catalog.db"
# [tenants.tenants.tenant1.custom_schemas]
# traces = "custom_traces_schema_definition"
# logs = "custom_logs_schema_definition"

# Environment variable examples:
# SIGNALDB_DATABASE_DSN=postgres://user:pass@localhost/signaldb
# SIGNALDB_STORAGE_DSN=file:///data/signaldb
# SIGNALDB_SCHEMA_CATALOG_TYPE=sql
# SIGNALDB_SCHEMA_CATALOG_URI=postgres://user:pass@localhost:5432/iceberg
# SIGNALDB_TENANTS_DEFAULT_TENANT=my_tenant

# Production environment example with Iceberg:
# SIGNALDB_DATABASE_DSN=postgres://signaldb:password@postgres:5432/signaldb_metadata
# SIGNALDB_STORAGE_DSN=s3://my-data-bucket/iceberg-tables/
# SIGNALDB_SCHEMA_CATALOG_TYPE=sql  
# SIGNALDB_SCHEMA_CATALOG_URI=postgres://iceberg:password@postgres:5432/iceberg_catalog
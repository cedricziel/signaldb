# SignalDB Distribution Configuration
# Copy to signaldb.toml and adjust for your environment

[database]
# Database for internal metadata storage
dsn = "sqlite://.data/signaldb.db"
# dsn = "postgres://user:password@localhost/signaldb"

[auth]
# Multi-Tenancy Authentication Configuration
# Controls API key validation and tenant isolation
enabled = true  # Set to false to disable authentication (dev/testing only)

# Admin API key for the /api/v1/admin/* management endpoints
# Generate a strong random key for production use
# admin_api_key = "sk-admin-your-secret-key-here"

# Tenant definitions with API keys and datasets
# Each tenant is isolated with separate WAL paths and Iceberg catalogs

# Example tenant: Acme Corporation
[[auth.tenants]]
id = "acme"
slug = "acme"
name = "Acme Corporation"
default_dataset = "production"  # Used when X-Dataset-ID header is not provided

# API keys for the Acme tenant
# Each key can have a descriptive name for auditing
[[auth.tenants.api_keys]]
key = "sk-acme-prod-key-123"       # The actual API key used in Authorization: Bearer header
name = "Production Key"             # Human-readable name for this key

[[auth.tenants.api_keys]]
key = "sk-acme-staging-key-456"
name = "Staging Key"

# Datasets for the Acme tenant
# Tenants can have multiple datasets for environment isolation
[[auth.tenants.datasets]]
id = "production"
slug = "production"
is_default = true

[[auth.tenants.datasets]]
id = "staging"
slug = "staging"
is_default = false

# Example tenant: Beta Inc
[[auth.tenants]]
id = "beta"
slug = "beta"
name = "Beta Inc"
default_dataset = "default"

[[auth.tenants.api_keys]]
key = "sk-beta-key-789"
name = "Beta Production Key"

[[auth.tenants.datasets]]
id = "default"
slug = "default"
is_default = true

# Optional: Custom schema configuration per tenant
# [[auth.tenants.schema_config]]
# catalog_type = "sql"
# catalog_uri = "sqlite://.data/acme_catalog.db"

# Authentication Headers Required:
# - Authorization: Bearer <api-key>
# - X-Tenant-ID: <tenant-id>
# - X-Dataset-ID: <dataset-id> (optional, uses default_dataset if not provided)
#
# Example authenticated OTLP request:
#   curl -X POST http://localhost:4318/v1/traces \
#     -H "Authorization: Bearer sk-acme-prod-key-123" \
#     -H "X-Tenant-ID: acme" \
#     -H "X-Dataset-ID: production" \
#     -H "Content-Type: application/json" \
#     -d @traces.json

[storage]
# Object storage for data files (Parquet, etc.)
dsn = "file:///.data/storage"
# dsn = "memory://"                    # In-memory storage for testing
# dsn = "s3://my-bucket/signaldb/"     # S3 storage (future)

[schema]
# Iceberg catalog configuration for table metadata
catalog_type = "sql"                   # sql (recommended) or memory
catalog_uri = "sqlite::memory:"        # In-memory SQLite catalog
# catalog_uri = "sqlite://.data/catalog.db"  # Persistent SQLite catalog
# catalog_uri = "postgres://user:password@localhost:5432/iceberg"  # Production PostgreSQL

# Default schemas to create for new tenants
[schema.default_schemas]
traces_enabled = true
logs_enabled = true
metrics_enabled = true

# Custom schema definitions (optional)
# [schema.default_schemas.custom_schemas]
# my_custom_table = '''{"fields": [...]}'''

# Advanced Iceberg Configuration (all settings are optional and auto-optimized)
# These provide fine-grained control for specialized workloads

# [schema.optimization]
# # Batch processing optimization (automatic by default)
# max_rows_per_batch = 50000           # Maximum rows per batch before splitting
# max_memory_per_batch_mb = 128        # Maximum memory per batch in MB
# enable_auto_split = true             # Automatically split oversized batches
# target_concurrent_batches = 4        # Number of batches to process concurrently
# 
# # Catalog caching for performance (enabled by default)
# enable_catalog_caching = true        # Cache catalog connections
# catalog_cache_ttl_seconds = 300      # Cache TTL in seconds (5 minutes)
#
# # Connection pooling for production (automatic by default)
# min_connections = 2                  # Minimum pool connections
# max_connections = 10                 # Maximum pool connections
# connection_timeout_ms = 5000         # Connection timeout
# idle_timeout_seconds = 300           # Idle connection timeout
# max_lifetime_seconds = 1800          # Connection max lifetime
#
# # Retry logic for reliability (enabled by default)
# max_retry_attempts = 3               # Maximum retry attempts
# initial_retry_delay_ms = 100         # Initial retry delay
# max_retry_delay_seconds = 5          # Maximum retry delay
# retry_backoff_multiplier = 2.0       # Exponential backoff multiplier

[discovery]
# Service discovery configuration
dsn = "sqlite::memory:"               # In-memory service registry
heartbeat_interval = "30s"
poll_interval = "60s"
ttl = "300s"

[wal]
# Write-Ahead Log configuration
wal_dir = ".data/wal"
max_segment_size = 67108864           # 64MB
max_buffer_entries = 1000
flush_interval = "30s"
max_buffer_size_bytes = 134217728     # 128MB

[compactor]
# Compactor Service Configuration
# The compactor manages the complete data lifecycle with three phases:
#   Phase 1: Dry-run compaction planning
#   Phase 2: Active Parquet file compaction for storage efficiency
#   Phase 3: Retention enforcement and lifecycle management (NEW)
#
# Phase 2: Compaction Settings
enabled = false                       # Enable compactor service (opt-in)
tick_interval = "5m"                  # Interval between compaction planning cycles
target_file_size_mb = 128             # Target file size after compaction (MB)
file_count_threshold = 5              # Minimum files to trigger compaction
min_input_file_size_kb = 1024         # Minimum input file size for compaction (KB)
max_files_per_job = 20                # Maximum files per compaction job

# Phase 3: Retention Enforcement Configuration
# Automatically drops expired partitions based on configurable retention policies.
# Uses a 3-tier hierarchy: Global defaults → Tenant overrides → Dataset overrides.
#
# IMPORTANT: Always test with dry_run = true first!
[compactor.retention]
enabled = false                       # Enable retention enforcement (opt-in, default: false)
dry_run = true                        # Log actions without executing (safe default: true)
retention_check_interval = "1h"       # Interval between retention checks (default: 1h)
grace_period = "1h"                   # Safety margin before cutoff (prevents clock skew issues)
timezone = "UTC"                      # Timezone for logging (internal storage uses UTC)
snapshots_to_keep = 5                 # Keep last N snapshots per table (minimum: 1)

# Global Default Retention Periods (per signal type)
# These apply to all tenants/datasets unless overridden below.
traces = "7d"                         # Trace data retention (default: 7 days)
logs = "30d"                          # Log data retention (default: 30 days)
metrics = "90d"                       # Metric data retention (default: 90 days)

# Tenant-Specific Retention Overrides (optional)
# Override global defaults for specific tenants.
# Use array-of-tables syntax: [[compactor.retention.tenant_overrides]]
#
# Example: Production tenant with custom retention (mixed overrides)
# [[compactor.retention.tenant_overrides]]
# tenant_id = "production"
# traces = "30d"                      # Override: Keep production traces for 30 days
# logs = "7d"                         # Override: Keep production logs for 7 days
# metrics = "90d"                     # Override: Keep production metrics for 90 days
#
# # Dataset-Specific Retention Overrides (highest priority)
# # Override tenant defaults for specific datasets within a tenant.
# [[compactor.retention.tenant_overrides.dataset_overrides]]
# dataset_id = "critical"
# traces = "90d"                      # Override: Keep critical traces for 90 days
# logs = "14d"                        # Override: Keep critical logs for 14 days
#
# [[compactor.retention.tenant_overrides.dataset_overrides]]
# dataset_id = "staging"
# traces = "1d"                       # Override: Keep staging traces for only 1 day
# logs = "1d"                         # Override: Keep staging logs for only 1 day
#
# Resolution Example:
#   Global:    traces = 7 days
#   Tenant:    traces = 30 days (overrides global)
#   Dataset:   traces = 90 days (overrides tenant)
# Result:
#   - production/critical → 90 days (dataset override)
#   - production/staging  → 1 day (dataset override)
#   - production/default  → 30 days (tenant override)
#   - other/any          → 7 days (global default)

# Phase 3: Orphan File Cleanup Configuration
# Automatically detects and deletes unreferenced Parquet files to reclaim storage.
# Uses a 4-phase detection algorithm with safety-first design.
#
# IMPORTANT: Always test with dry_run = true first!
[compactor.orphan_cleanup]
enabled = false                       # Enable orphan cleanup (opt-in, default: false)
dry_run = true                        # Log orphans without deleting (safe default: true)
cleanup_interval_hours = 24           # Run cleanup every N hours (default: 24, once per day)

# Safety Settings (conservative defaults)
grace_period_hours = 24               # Don't delete files younger than this (default: 24h)
                                      # Protects against in-flight writes and race conditions
revalidate_before_delete = true       # Re-check file status before deletion (recommended: true)
                                      # Adds ~10% overhead but prevents accidental deletion
max_snapshot_age_hours = 720          # Consider snapshots within last N hours (default: 720 = 30 days)
                                      # Files referenced by older snapshots may be incorrectly flagged

# Performance Settings
batch_size = 1000                     # Process N files per batch (default: 1000)
                                      # Smaller = more checkpoints, larger = faster processing
rate_limit_delay_ms = 0               # Delay between batches in milliseconds (default: 0)
                                      # Use if hitting object store rate limits (S3 throttling)

# Orphan Cleanup Safety Explained:
# 1. Grace Period: Files younger than grace_period_hours are NEVER deleted
# 2. Revalidation: Before deletion, re-check if file is still orphaned (catches races)
# 3. Snapshot Window: Only scan recent snapshots to reduce memory usage
# 4. Batch Processing: Process in batches with progress tracking for resumability
#
# Tuning Guidance:
# - High-volume systems: increase batch_size (2000-5000), add rate_limit_delay_ms
# - Low-memory systems: decrease batch_size (250-500), reduce max_snapshot_age_hours
# - Aggressive cleanup: reduce grace_period_hours (12h), reduce cleanup_interval_hours (12h for more frequent runs)

[tenants]
# Multi-tenant configuration (always active)
default_tenant = "default"

# Example tenant configuration
# [tenants.tenants.tenant1]
# enabled = true
# [tenants.tenants.tenant1.schema]
# catalog_type = "sql"
# catalog_uri = "sqlite://.data/tenant1_catalog.db"
# [tenants.tenants.tenant1.custom_schemas]
# traces = "custom_traces_schema_definition"
# logs = "custom_logs_schema_definition"

# Environment variable examples:
# SIGNALDB_DATABASE_DSN=postgres://user:pass@localhost/signaldb
# SIGNALDB_STORAGE_DSN=file:///data/signaldb
# SIGNALDB_SCHEMA_CATALOG_TYPE=sql
# SIGNALDB_SCHEMA_CATALOG_URI=postgres://user:pass@localhost:5432/iceberg
# SIGNALDB_TENANTS_DEFAULT_TENANT=my_tenant

# Production environment example with Iceberg:
# SIGNALDB_DATABASE_DSN=postgres://signaldb:password@postgres:5432/signaldb_metadata
# SIGNALDB_STORAGE_DSN=s3://my-data-bucket/iceberg-tables/
# SIGNALDB_SCHEMA_CATALOG_TYPE=sql  
# SIGNALDB_SCHEMA_CATALOG_URI=postgres://iceberg:password@postgres:5432/iceberg_catalog